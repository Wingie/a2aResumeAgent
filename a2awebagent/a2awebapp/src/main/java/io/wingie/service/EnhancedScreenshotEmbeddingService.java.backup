package io.wingie.service;

import io.wingie.entity.neo4j.ScreenshotNode;
import io.wingie.repository.neo4j.ScreenshotNodeRepository;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.JsonNode;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.awt.image.BufferedImage;
import java.awt.Color;
import java.awt.Graphics2D;
import java.awt.RenderingHints;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.net.URI;
import java.net.http.HttpClient;
import java.net.http.HttpRequest;
import java.net.http.HttpResponse;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.stream.Collectors;
import javax.imageio.ImageIO;

/**
 * Enhanced screenshot embedding service with real CLIP model integration
 * Provides comprehensive visual similarity search and embedding generation
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class EnhancedScreenshotEmbeddingService {
    
    private final ScreenshotNodeRepository screenshotRepository;
    private final ObjectMapper objectMapper;
    private final HttpClient httpClient = HttpClient.newBuilder()
        .connectTimeout(Duration.ofSeconds(30))
        .build();
    
    @Value("${embedding.huggingface.api-key:}")
    private String huggingFaceApiKey;
    
    @Value("${embedding.clip.model:openai/clip-vit-base-patch32}")
    private String clipModelName;
    
    @Value("${embedding.service.url:https://api-inference.huggingface.co/models/}")
    private String embeddingServiceUrl;
    
    @Value("${screenshot.embedding.enabled:true}")
    private boolean embeddingEnabled;
    
    @Value("${screenshot.embedding.batch-size:10}")
    private int batchSize;
    
    @Value("${screenshot.similarity.threshold:0.8}")
    private double similarityThreshold;
    
    /**
     * Generate comprehensive embeddings for a screenshot with enhanced capabilities
     */
    @Async
    @Transactional
    public CompletableFuture<Void> generateEnhancedEmbeddings(ScreenshotNode screenshot) {
        if (!embeddingEnabled) {
            log.debug("Screenshot embedding generation disabled");
            return CompletableFuture.completedFuture(null);
        }
        
        try {
            log.info("Generating enhanced embeddings for screenshot: {}", screenshot.getScreenshotPath());
            
            // Load image from base64 or file path
            BufferedImage image = loadImage(screenshot);
            if (image == null) {
                log.warn("Failed to load image for embedding generation");
                return CompletableFuture.completedFuture(null);
            }
            
            // Generate CLIP embedding using HuggingFace API
            String clipEmbedding = generateRealCLIPEmbedding(image);
            if (clipEmbedding != null) {
                screenshot.setClipEmbedding(clipEmbedding);
                log.info("✅ Generated CLIP embedding for screenshot");
            }
            
            // Generate enhanced visual features
            String visualFeatures = generateEnhancedVisualFeatures(image);
            screenshot.setVisualFeaturesEmbedding(visualFeatures);
            
            // Extract detailed color palette
            String colorPalette = extractEnhancedColorPalette(image);
            screenshot.setColorPalette(colorPalette);
            
            // Generate perceptual hash for duplicate detection
            String perceptualHash = generatePerceptualHash(image);
            screenshot.setImageHash(perceptualHash);
            
            // Extract UI patterns and layout features
            String uiPatterns = extractUIPatterns(image);
            screenshot.setUiPatterns(uiPatterns);
            
            // Generate semantic description using visual analysis
            String visualDescription = generateVisualDescription(image);
            screenshot.setVisualAnalysis(visualDescription);
            
            // Set embedding version and metadata
            screenshot.setEmbeddingVersion("v2.0_enhanced");
            screenshot.setQualityScore(calculateImageQuality(image));
            
            // Save enhanced screenshot
            screenshotRepository.save(screenshot);
            
            // Find and create similarity relationships
            findAndCreateSimilarityRelationships(screenshot);
            
            log.info("✅ Enhanced embedding generation completed for screenshot: {}", screenshot.getScreenshotPath());
            
        } catch (Exception e) {
            log.error("Failed to generate enhanced embeddings for screenshot: {}", e.getMessage(), e);
        }
        
        return CompletableFuture.completedFuture(null);
    }
    
    /**
     * Generate real CLIP embedding using HuggingFace Inference API
     */
    private String generateRealCLIPEmbedding(BufferedImage image) {
        if (huggingFaceApiKey == null || huggingFaceApiKey.isEmpty()) {
            log.warn("HuggingFace API key not configured, using enhanced visual features instead");
            return generateFallbackEmbedding(image);\n        }\n        \n        try {\n            // Prepare image for CLIP model (224x224 is standard)\n            BufferedImage resizedImage = resizeImage(image, 224, 224);\n            \n            // Convert to base64\n            ByteArrayOutputStream baos = new ByteArrayOutputStream();\n            ImageIO.write(resizedImage, \"png\", baos);\n            String base64Image = Base64.getEncoder().encodeToString(baos.toByteArray());\n            \n            // Create request payload\n            Map<String, Object> payload = new HashMap<>();\n            payload.put(\"inputs\", base64Image);\n            payload.put(\"options\", Map.of(\"wait_for_model\", true));\n            \n            String requestBody = objectMapper.writeValueAsString(payload);\n            \n            // Make API request\n            HttpRequest request = HttpRequest.newBuilder()\n                .uri(URI.create(embeddingServiceUrl + clipModelName))\n                .header(\"Authorization\", \"Bearer \" + huggingFaceApiKey)\n                .header(\"Content-Type\", \"application/json\")\n                .POST(HttpRequest.BodyPublishers.ofString(requestBody))\n                .build();\n            \n            HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());\n            \n            if (response.statusCode() == 200) {\n                // Parse response and extract embedding\n                JsonNode responseJson = objectMapper.readTree(response.body());\n                \n                // HuggingFace returns embeddings in different formats depending on model\n                // For CLIP, it's typically a float array\n                if (responseJson.isArray() && responseJson.size() > 0) {\n                    JsonNode embedding = responseJson.get(0);\n                    if (embedding.isArray()) {\n                        float[] embeddingArray = new float[embedding.size()];\n                        for (int i = 0; i < embedding.size(); i++) {\n                            embeddingArray[i] = (float) embedding.get(i).asDouble();\n                        }\n                        return encodeFloatArrayToBase64(embeddingArray);\n                    }\n                }\n                \n                log.warn(\"Unexpected response format from HuggingFace API\");\n                return generateFallbackEmbedding(image);\n                \n            } else {\n                log.warn(\"HuggingFace API returned status {}: {}\", response.statusCode(), response.body());\n                return generateFallbackEmbedding(image);\n            }\n            \n        } catch (Exception e) {\n            log.error(\"Failed to generate CLIP embedding via HuggingFace API: {}\", e.getMessage());\n            return generateFallbackEmbedding(image);\n        }\n    }\n    \n    /**\n     * Generate fallback embedding using enhanced visual features\n     */\n    private String generateFallbackEmbedding(BufferedImage image) {\n        try {\n            // Create a comprehensive visual feature vector\n            List<Float> features = new ArrayList<>();\n            \n            // Add color histogram features (64 bins)\n            features.addAll(extractColorHistogram(image));\n            \n            // Add texture features (16 features)\n            features.addAll(extractTextureFeatures(image));\n            \n            // Add edge density features (16 features)\n            features.addAll(extractEdgeFeatures(image));\n            \n            // Add spatial layout features (32 features)\n            features.addAll(extractSpatialFeatures(image));\n            \n            // Pad to 512 dimensions to match CLIP\n            while (features.size() < 512) {\n                features.add(0.0f);\n            }\n            \n            // Convert to array and normalize\n            float[] featureArray = new float[512];\n            for (int i = 0; i < 512; i++) {\n                featureArray[i] = i < features.size() ? features.get(i) : 0.0f;\n            }\n            \n            // Normalize vector\n            normalizeVector(featureArray);\n            \n            return encodeFloatArrayToBase64(featureArray);\n            \n        } catch (Exception e) {\n            log.error(\"Failed to generate fallback embedding: {}\", e.getMessage());\n            return null;\n        }\n    }\n    \n    /**\n     * Extract color histogram features\n     */\n    private List<Float> extractColorHistogram(BufferedImage image) {\n        List<Float> features = new ArrayList<>();\n        \n        // Create 4x4x4 RGB histogram (64 bins)\n        int[][][] histogram = new int[4][4][4];\n        int width = image.getWidth();\n        int height = image.getHeight();\n        \n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                Color color = new Color(image.getRGB(x, y));\n                int rBin = (color.getRed() * 4) / 256;\n                int gBin = (color.getGreen() * 4) / 256;\n                int bBin = (color.getBlue() * 4) / 256;\n                \n                // Clamp to valid range\n                rBin = Math.min(3, Math.max(0, rBin));\n                gBin = Math.min(3, Math.max(0, gBin));\n                bBin = Math.min(3, Math.max(0, bBin));\n                \n                histogram[rBin][gBin][bBin]++;\n            }\n        }\n        \n        // Normalize and flatten histogram\n        int totalPixels = width * height;\n        for (int r = 0; r < 4; r++) {\n            for (int g = 0; g < 4; g++) {\n                for (int b = 0; b < 4; b++) {\n                    features.add((float) histogram[r][g][b] / totalPixels);\n                }\n            }\n        }\n        \n        return features;\n    }\n    \n    /**\n     * Extract texture features using local binary patterns\n     */\n    private List<Float> extractTextureFeatures(BufferedImage image) {\n        List<Float> features = new ArrayList<>();\n        \n        // Convert to grayscale\n        BufferedImage grayImage = convertToGrayscale(image);\n        int width = grayImage.getWidth();\n        int height = grayImage.getHeight();\n        \n        // Calculate texture measures in 4x4 grid\n        for (int gridY = 0; gridY < 4; gridY++) {\n            for (int gridX = 0; gridX < 4; gridX++) {\n                int startX = (gridX * width) / 4;\n                int endX = ((gridX + 1) * width) / 4;\n                int startY = (gridY * height) / 4;\n                int endY = ((gridY + 1) * height) / 4;\n                \n                // Calculate variance in this region\n                double sum = 0;\n                double sumSquares = 0;\n                int count = 0;\n                \n                for (int y = startY; y < endY; y++) {\n                    for (int x = startX; x < endX; x++) {\n                        int gray = grayImage.getRGB(x, y) & 0xFF;\n                        sum += gray;\n                        sumSquares += gray * gray;\n                        count++;\n                    }\n                }\n                \n                double mean = sum / count;\n                double variance = (sumSquares / count) - (mean * mean);\n                features.add((float) Math.sqrt(variance) / 255.0f);\n            }\n        }\n        \n        return features;\n    }\n    \n    /**\n     * Extract edge density features\n     */\n    private List<Float> extractEdgeFeatures(BufferedImage image) {\n        List<Float> features = new ArrayList<>();\n        \n        BufferedImage grayImage = convertToGrayscale(image);\n        int width = grayImage.getWidth();\n        int height = grayImage.getHeight();\n        \n        // Simple edge detection using Sobel-like operator\n        int[][] edges = new int[height][width];\n        \n        for (int y = 1; y < height - 1; y++) {\n            for (int x = 1; x < width - 1; x++) {\n                int tl = grayImage.getRGB(x-1, y-1) & 0xFF;\n                int tm = grayImage.getRGB(x, y-1) & 0xFF;\n                int tr = grayImage.getRGB(x+1, y-1) & 0xFF;\n                int ml = grayImage.getRGB(x-1, y) & 0xFF;\n                int mr = grayImage.getRGB(x+1, y) & 0xFF;\n                int bl = grayImage.getRGB(x-1, y+1) & 0xFF;\n                int bm = grayImage.getRGB(x, y+1) & 0xFF;\n                int br = grayImage.getRGB(x+1, y+1) & 0xFF;\n                \n                int gx = (tr + 2*mr + br) - (tl + 2*ml + bl);\n                int gy = (bl + 2*bm + br) - (tl + 2*tm + tr);\n                \n                edges[y][x] = (int) Math.sqrt(gx*gx + gy*gy);\n            }\n        }\n        \n        // Calculate edge density in 4x4 grid\n        for (int gridY = 0; gridY < 4; gridY++) {\n            for (int gridX = 0; gridX < 4; gridX++) {\n                int startX = (gridX * width) / 4;\n                int endX = ((gridX + 1) * width) / 4;\n                int startY = (gridY * height) / 4;\n                int endY = ((gridY + 1) * height) / 4;\n                \n                int edgeCount = 0;\n                int totalPixels = 0;\n                \n                for (int y = startY; y < endY; y++) {\n                    for (int x = startX; x < endX; x++) {\n                        if (edges[y][x] > 50) { // Edge threshold\n                            edgeCount++;\n                        }\n                        totalPixels++;\n                    }\n                }\n                \n                features.add((float) edgeCount / totalPixels);\n            }\n        }\n        \n        return features;\n    }\n    \n    /**\n     * Extract spatial layout features\n     */\n    private List<Float> extractSpatialFeatures(BufferedImage image) {\n        List<Float> features = new ArrayList<>();\n        \n        int width = image.getWidth();\n        int height = image.getHeight();\n        \n        // Calculate brightness distribution in 8x4 grid\n        for (int gridY = 0; gridY < 4; gridY++) {\n            for (int gridX = 0; gridX < 8; gridX++) {\n                int startX = (gridX * width) / 8;\n                int endX = ((gridX + 1) * width) / 8;\n                int startY = (gridY * height) / 4;\n                int endY = ((gridY + 1) * height) / 4;\n                \n                double avgBrightness = 0;\n                int count = 0;\n                \n                for (int y = startY; y < endY; y++) {\n                    for (int x = startX; x < endX; x++) {\n                        Color color = new Color(image.getRGB(x, y));\n                        double brightness = (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n                        avgBrightness += brightness;\n                        count++;\n                    }\n                }\n                \n                features.add((float) (avgBrightness / count / 255.0));\n            }\n        }\n        \n        return features;\n    }\n    \n    /**\n     * Generate enhanced visual features with detailed analysis\n     */\n    private String generateEnhancedVisualFeatures(BufferedImage image) {\n        try {\n            Map<String, Object> features = new HashMap<>();\n            \n            // Image dimensions and aspect ratio\n            features.put(\"width\", image.getWidth());\n            features.put(\"height\", image.getHeight());\n            features.put(\"aspectRatio\", (double) image.getWidth() / image.getHeight());\n            \n            // Color analysis\n            features.put(\"avgBrightness\", calculateAverageBrightness(image));\n            features.put(\"colorVariance\", calculateColorVariance(image));\n            features.put(\"dominantColorCount\", countDominantColors(image));\n            \n            // Texture analysis\n            features.put(\"textureComplexity\", calculateTextureComplexity(image));\n            features.put(\"edgeDensity\", calculateEdgeDensity(image));\n            \n            // Layout analysis\n            features.put(\"centerWeightedBrightness\", calculateCenterWeightedBrightness(image));\n            features.put(\"symmetryScore\", calculateSymmetryScore(image));\n            \n            return objectMapper.writeValueAsString(features);\n            \n        } catch (Exception e) {\n            log.error(\"Failed to generate enhanced visual features: {}\", e.getMessage());\n            return \"{}\";\n        }\n    }\n    \n    /**\n     * Extract enhanced color palette with detailed analysis\n     */\n    private String extractEnhancedColorPalette(BufferedImage image) {\n        try {\n            Map<String, Integer> colorCounts = new HashMap<>();\n            int width = image.getWidth();\n            int height = image.getHeight();\n            \n            // Sample colors from image\n            for (int y = 0; y < height; y += 5) {\n                for (int x = 0; x < width; x += 5) {\n                    Color color = new Color(image.getRGB(x, y));\n                    // Quantize color to reduce noise\n                    int r = (color.getRed() / 32) * 32;\n                    int g = (color.getGreen() / 32) * 32;\n                    int b = (color.getBlue() / 32) * 32;\n                    \n                    String colorKey = String.format(\"#%02x%02x%02x\", r, g, b);\n                    colorCounts.put(colorKey, colorCounts.getOrDefault(colorKey, 0) + 1);\n                }\n            }\n            \n            // Get top 10 colors\n            List<String> topColors = colorCounts.entrySet().stream()\n                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())\n                .limit(10)\n                .map(Map.Entry::getKey)\n                .collect(Collectors.toList());\n            \n            Map<String, Object> palette = new HashMap<>();\n            palette.put(\"colors\", topColors);\n            palette.put(\"dominantColor\", topColors.isEmpty() ? \"#000000\" : topColors.get(0));\n            palette.put(\"colorDiversity\", colorCounts.size());\n            \n            return objectMapper.writeValueAsString(palette);\n            \n        } catch (Exception e) {\n            log.error(\"Failed to extract enhanced color palette: {}\", e.getMessage());\n            return \"{}\";\n        }\n    }\n    \n    /**\n     * Generate perceptual hash for duplicate detection\n     */\n    private String generatePerceptualHash(BufferedImage image) {\n        try {\n            // Resize to 8x8 for hash generation\n            BufferedImage small = resizeImage(image, 8, 8);\n            BufferedImage gray = convertToGrayscale(small);\n            \n            // Calculate average pixel value\n            int sum = 0;\n            for (int y = 0; y < 8; y++) {\n                for (int x = 0; x < 8; x++) {\n                    sum += (gray.getRGB(x, y) & 0xFF);\n                }\n            }\n            int average = sum / 64;\n            \n            // Generate hash string\n            StringBuilder hash = new StringBuilder();\n            for (int y = 0; y < 8; y++) {\n                for (int x = 0; x < 8; x++) {\n                    int pixel = gray.getRGB(x, y) & 0xFF;\n                    hash.append(pixel >= average ? '1' : '0');\n                }\n            }\n            \n            return hash.toString();\n            \n        } catch (Exception e) {\n            log.error(\"Failed to generate perceptual hash: {}\", e.getMessage());\n            return \"0000000000000000000000000000000000000000000000000000000000000000\";\n        }\n    }\n    \n    /**\n     * Extract UI patterns and interface elements\n     */\n    private String extractUIPatterns(BufferedImage image) {\n        try {\n            Map<String, Object> patterns = new HashMap<>();\n            \n            // Analyze layout patterns\n            patterns.put(\"hasHeader\", detectHeaderPattern(image));\n            patterns.put(\"hasFooter\", detectFooterPattern(image));\n            patterns.put(\"hasSidebar\", detectSidebarPattern(image));\n            patterns.put(\"hasModal\", detectModalPattern(image));\n            \n            // Analyze UI elements\n            patterns.put(\"buttonLikeness\", detectButtonPatterns(image));\n            patterns.put(\"formElements\", detectFormElements(image));\n            patterns.put(\"navigationElements\", detectNavigationElements(image));\n            \n            return objectMapper.writeValueAsString(patterns);\n            \n        } catch (Exception e) {\n            log.error(\"Failed to extract UI patterns: {}\", e.getMessage());\n            return \"{}\";\n        }\n    }\n    \n    /**\n     * Generate visual description for searchability\n     */\n    private String generateVisualDescription(BufferedImage image) {\n        try {\n            StringBuilder description = new StringBuilder();\n            \n            // Image characteristics\n            description.append(String.format(\"Image size: %dx%d. \", image.getWidth(), image.getHeight()));\n            \n            // Color analysis\n            double avgBrightness = calculateAverageBrightness(image);\n            if (avgBrightness > 200) {\n                description.append(\"Very bright image. \");\n            } else if (avgBrightness > 150) {\n                description.append(\"Bright image. \");\n            } else if (avgBrightness > 100) {\n                description.append(\"Medium brightness. \");\n            } else {\n                description.append(\"Dark image. \");\n            }\n            \n            // Layout analysis\n            if (detectHeaderPattern(image)) {\n                description.append(\"Contains header region. \");\n            }\n            if (detectModalPattern(image)) {\n                description.append(\"Contains modal dialog. \");\n            }\n            if (detectFormElements(image) > 0.3) {\n                description.append(\"Contains form elements. \");\n            }\n            \n            return description.toString().trim();\n            \n        } catch (Exception e) {\n            log.error(\"Failed to generate visual description: {}\", e.getMessage());\n            return \"Screenshot image\";\n        }\n    }\n    \n    /**\n     * Find and create similarity relationships with existing screenshots\n     */\n    private void findAndCreateSimilarityRelationships(ScreenshotNode screenshot) {\n        try {\n            if (screenshot.getClipEmbedding() == null) {\n                return;\n            }\n            \n            // Get all screenshots with embeddings\n            List<ScreenshotNode> existingScreenshots = screenshotRepository.findScreenshotsWithEmbeddings();\n            \n            float[] currentEmbedding = decodeBase64ToFloatArray(screenshot.getClipEmbedding());\n            \n            for (ScreenshotNode existing : existingScreenshots) {\n                if (existing.getId().equals(screenshot.getId())) {\n                    continue;\n                }\n                \n                if (existing.getClipEmbedding() != null) {\n                    float[] existingEmbedding = decodeBase64ToFloatArray(existing.getClipEmbedding());\n                    double similarity = calculateCosineSimilarity(currentEmbedding, existingEmbedding);\n                    \n                    if (similarity > similarityThreshold) {\n                        // Create similarity relationship\n                        log.info(\"Found similar screenshot: {} (similarity: {:.3f})\", \n                            existing.getScreenshotPath(), similarity);\n                        \n                        // Note: Actual relationship creation would depend on Neo4j repository method\n                        // screenshotRepository.createSimilarityRelationship(screenshot, existing, similarity);\n                    }\n                }\n            }\n            \n        } catch (Exception e) {\n            log.error(\"Failed to find similarity relationships: {}\", e.getMessage());\n        }\n    }\n    \n    // Helper methods\n    \n    private BufferedImage loadImage(ScreenshotNode screenshot) {\n        try {\n            if (screenshot.getBase64Data() != null && !screenshot.getBase64Data().isEmpty()) {\n                byte[] imageData = Base64.getDecoder().decode(screenshot.getBase64Data());\n                return ImageIO.read(new java.io.ByteArrayInputStream(imageData));\n            } else if (screenshot.getScreenshotPath() != null) {\n                Path imagePath = Paths.get(screenshot.getScreenshotPath());\n                if (Files.exists(imagePath)) {\n                    return ImageIO.read(imagePath.toFile());\n                }\n            }\n            return null;\n        } catch (Exception e) {\n            log.error(\"Failed to load image: {}\", e.getMessage());\n            return null;\n        }\n    }\n    \n    private BufferedImage resizeImage(BufferedImage original, int width, int height) {\n        BufferedImage resized = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);\n        Graphics2D g = resized.createGraphics();\n        g.setRenderingHint(RenderingHints.KEY_INTERPOLATION, RenderingHints.VALUE_INTERPOLATION_BILINEAR);\n        g.drawImage(original, 0, 0, width, height, null);\n        g.dispose();\n        return resized;\n    }\n    \n    private BufferedImage convertToGrayscale(BufferedImage original) {\n        BufferedImage grayscale = new BufferedImage(original.getWidth(), original.getHeight(), BufferedImage.TYPE_BYTE_GRAY);\n        Graphics2D g = grayscale.createGraphics();\n        g.drawImage(original, 0, 0, null);\n        g.dispose();\n        return grayscale;\n    }\n    \n    private void normalizeVector(float[] vector) {\n        double norm = 0;\n        for (float v : vector) {\n            norm += v * v;\n        }\n        norm = Math.sqrt(norm);\n        \n        if (norm > 0) {\n            for (int i = 0; i < vector.length; i++) {\n                vector[i] /= norm;\n            }\n        }\n    }\n    \n    private double calculateCosineSimilarity(float[] a, float[] b) {\n        if (a.length != b.length) {\n            return 0.0;\n        }\n        \n        double dotProduct = 0;\n        double normA = 0;\n        double normB = 0;\n        \n        for (int i = 0; i < a.length; i++) {\n            dotProduct += a[i] * b[i];\n            normA += a[i] * a[i];\n            normB += b[i] * b[i];\n        }\n        \n        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n    }\n    \n    private String encodeFloatArrayToBase64(float[] array) {\n        try {\n            ByteArrayOutputStream baos = new ByteArrayOutputStream();\n            for (float f : array) {\n                int bits = Float.floatToIntBits(f);\n                baos.write((bits >>> 24) & 0xFF);\n                baos.write((bits >>> 16) & 0xFF);\n                baos.write((bits >>> 8) & 0xFF);\n                baos.write(bits & 0xFF);\n            }\n            return Base64.getEncoder().encodeToString(baos.toByteArray());\n        } catch (Exception e) {\n            log.error(\"Failed to encode float array to base64: {}\", e.getMessage());\n            return \"\";\n        }\n    }\n    \n    private float[] decodeBase64ToFloatArray(String base64) {\n        try {\n            byte[] bytes = Base64.getDecoder().decode(base64);\n            float[] floats = new float[bytes.length / 4];\n            \n            for (int i = 0; i < floats.length; i++) {\n                int bits = ((bytes[i * 4] & 0xFF) << 24) |\n                          ((bytes[i * 4 + 1] & 0xFF) << 16) |\n                          ((bytes[i * 4 + 2] & 0xFF) << 8) |\n                          (bytes[i * 4 + 3] & 0xFF);\n                floats[i] = Float.intBitsToFloat(bits);\n            }\n            \n            return floats;\n        } catch (Exception e) {\n            log.error(\"Failed to decode base64 to float array: {}\", e.getMessage());\n            return new float[0];\n        }\n    }\n    \n    // Image analysis helper methods\n    \n    private double calculateAverageBrightness(BufferedImage image) {\n        long sum = 0;\n        int count = 0;\n        \n        for (int y = 0; y < image.getHeight(); y++) {\n            for (int x = 0; x < image.getWidth(); x++) {\n                Color color = new Color(image.getRGB(x, y));\n                sum += (color.getRed() + color.getGreen() + color.getBlue()) / 3;\n                count++;\n            }\n        }\n        \n        return (double) sum / count;\n    }\n    \n    private double calculateColorVariance(BufferedImage image) {\n        double mean = calculateAverageBrightness(image);\n        double sum = 0;\n        int count = 0;\n        \n        for (int y = 0; y < image.getHeight(); y++) {\n            for (int x = 0; x < image.getWidth(); x++) {\n                Color color = new Color(image.getRGB(x, y));\n                double brightness = (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n                sum += Math.pow(brightness - mean, 2);\n                count++;\n            }\n        }\n        \n        return sum / count;\n    }\n    \n    private int countDominantColors(BufferedImage image) {\n        Set<Integer> colors = new HashSet<>();\n        \n        // Sample every 5th pixel to reduce computation\n        for (int y = 0; y < image.getHeight(); y += 5) {\n            for (int x = 0; x < image.getWidth(); x += 5) {\n                Color color = new Color(image.getRGB(x, y));\n                // Quantize color to reduce noise\n                int r = (color.getRed() / 32) * 32;\n                int g = (color.getGreen() / 32) * 32;\n                int b = (color.getBlue() / 32) * 32;\n                colors.add((r << 16) | (g << 8) | b);\n            }\n        }\n        \n        return colors.size();\n    }\n    \n    private double calculateTextureComplexity(BufferedImage image) {\n        // Simplified texture complexity using local variance\n        BufferedImage gray = convertToGrayscale(image);\n        double totalVariance = 0;\n        int regions = 0;\n        \n        int regionSize = 16;\n        for (int y = 0; y < gray.getHeight() - regionSize; y += regionSize) {\n            for (int x = 0; x < gray.getWidth() - regionSize; x += regionSize) {\n                double sum = 0;\n                double sumSquares = 0;\n                int count = 0;\n                \n                for (int dy = 0; dy < regionSize; dy++) {\n                    for (int dx = 0; dx < regionSize; dx++) {\n                        int pixel = gray.getRGB(x + dx, y + dy) & 0xFF;\n                        sum += pixel;\n                        sumSquares += pixel * pixel;\n                        count++;\n                    }\n                }\n                \n                double mean = sum / count;\n                double variance = (sumSquares / count) - (mean * mean);\n                totalVariance += variance;\n                regions++;\n            }\n        }\n        \n        return totalVariance / regions;\n    }\n    \n    private double calculateEdgeDensity(BufferedImage image) {\n        BufferedImage gray = convertToGrayscale(image);\n        int edges = 0;\n        int total = 0;\n        \n        for (int y = 1; y < gray.getHeight() - 1; y++) {\n            for (int x = 1; x < gray.getWidth() - 1; x++) {\n                int center = gray.getRGB(x, y) & 0xFF;\n                int left = gray.getRGB(x - 1, y) & 0xFF;\n                int right = gray.getRGB(x + 1, y) & 0xFF;\n                int top = gray.getRGB(x, y - 1) & 0xFF;\n                int bottom = gray.getRGB(x, y + 1) & 0xFF;\n                \n                int gradient = Math.abs(right - left) + Math.abs(bottom - top);\n                if (gradient > 50) {\n                    edges++;\n                }\n                total++;\n            }\n        }\n        \n        return (double) edges / total;\n    }\n    \n    private double calculateCenterWeightedBrightness(BufferedImage image) {\n        double totalWeight = 0;\n        double weightedSum = 0;\n        int centerX = image.getWidth() / 2;\n        int centerY = image.getHeight() / 2;\n        \n        for (int y = 0; y < image.getHeight(); y++) {\n            for (int x = 0; x < image.getWidth(); x++) {\n                double distance = Math.sqrt(Math.pow(x - centerX, 2) + Math.pow(y - centerY, 2));\n                double weight = 1.0 / (1.0 + distance / 100.0);\n                \n                Color color = new Color(image.getRGB(x, y));\n                double brightness = (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n                \n                weightedSum += brightness * weight;\n                totalWeight += weight;\n            }\n        }\n        \n        return weightedSum / totalWeight;\n    }\n    \n    private double calculateSymmetryScore(BufferedImage image) {\n        int width = image.getWidth();\n        int height = image.getHeight();\n        double symmetryScore = 0;\n        int comparisons = 0;\n        \n        // Check horizontal symmetry\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width / 2; x++) {\n                Color left = new Color(image.getRGB(x, y));\n                Color right = new Color(image.getRGB(width - 1 - x, y));\n                \n                double similarity = 1.0 - (Math.abs(left.getRed() - right.getRed()) + \n                                          Math.abs(left.getGreen() - right.getGreen()) + \n                                          Math.abs(left.getBlue() - right.getBlue())) / (3.0 * 255.0);\n                \n                symmetryScore += similarity;\n                comparisons++;\n            }\n        }\n        \n        return symmetryScore / comparisons;\n    }\n    \n    private double calculateImageQuality(BufferedImage image) {\n        double edgeDensity = calculateEdgeDensity(image);\n        double colorVariance = calculateColorVariance(image);\n        double textureComplexity = calculateTextureComplexity(image);\n        \n        // Combine metrics into quality score (0-1)\n        double quality = (edgeDensity * 0.3) + (Math.min(colorVariance / 10000, 1.0) * 0.3) + \n                        (Math.min(textureComplexity / 1000, 1.0) * 0.4);\n        \n        return Math.min(1.0, Math.max(0.0, quality));\n    }\n    \n    // UI pattern detection methods\n    \n    private boolean detectHeaderPattern(BufferedImage image) {\n        // Simplified header detection - look for horizontal bright region at top\n        int height = image.getHeight();\n        int width = image.getWidth();\n        double topBrightness = 0;\n        \n        for (int y = 0; y < height / 10; y++) {\n            for (int x = 0; x < width; x++) {\n                Color color = new Color(image.getRGB(x, y));\n                topBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n            }\n        }\n        \n        topBrightness /= (width * height / 10);\n        double avgBrightness = calculateAverageBrightness(image);\n        \n        return topBrightness > avgBrightness * 1.2;\n    }\n    \n    private boolean detectFooterPattern(BufferedImage image) {\n        // Similar to header but at bottom\n        int height = image.getHeight();\n        int width = image.getWidth();\n        double bottomBrightness = 0;\n        \n        for (int y = height - height / 10; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                Color color = new Color(image.getRGB(x, y));\n                bottomBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n            }\n        }\n        \n        bottomBrightness /= (width * height / 10);\n        double avgBrightness = calculateAverageBrightness(image);\n        \n        return bottomBrightness > avgBrightness * 1.2;\n    }\n    \n    private boolean detectSidebarPattern(BufferedImage image) {\n        // Look for vertical bright region on left or right\n        int height = image.getHeight();\n        int width = image.getWidth();\n        double leftBrightness = 0;\n        double rightBrightness = 0;\n        \n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width / 10; x++) {\n                Color color = new Color(image.getRGB(x, y));\n                leftBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n            }\n            for (int x = width - width / 10; x < width; x++) {\n                Color color = new Color(image.getRGB(x, y));\n                rightBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n            }\n        }\n        \n        leftBrightness /= (width / 10 * height);\n        rightBrightness /= (width / 10 * height);\n        double avgBrightness = calculateAverageBrightness(image);\n        \n        return leftBrightness > avgBrightness * 1.2 || rightBrightness > avgBrightness * 1.2;\n    }\n    \n    private boolean detectModalPattern(BufferedImage image) {\n        // Look for dark overlay with bright center region\n        int height = image.getHeight();\n        int width = image.getWidth();\n        \n        // Check if center is brighter than edges\n        double centerBrightness = 0;\n        double edgeBrightness = 0;\n        \n        int centerRegionSize = Math.min(width, height) / 3;\n        int centerX = width / 2;\n        int centerY = height / 2;\n        \n        // Sample center region\n        for (int y = centerY - centerRegionSize / 2; y < centerY + centerRegionSize / 2; y++) {\n            for (int x = centerX - centerRegionSize / 2; x < centerX + centerRegionSize / 2; x++) {\n                if (y >= 0 && y < height && x >= 0 && x < width) {\n                    Color color = new Color(image.getRGB(x, y));\n                    centerBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n                }\n            }\n        }\n        centerBrightness /= (centerRegionSize * centerRegionSize);\n        \n        // Sample edge regions\n        int edgePixels = 0;\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                if (x < width / 10 || x > width * 9 / 10 || y < height / 10 || y > height * 9 / 10) {\n                    Color color = new Color(image.getRGB(x, y));\n                    edgeBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n                    edgePixels++;\n                }\n            }\n        }\n        edgeBrightness /= edgePixels;\n        \n        return centerBrightness > edgeBrightness * 1.5;\n    }\n    \n    private double detectButtonPatterns(BufferedImage image) {\n        // Simplified button detection - look for rectangular regions with consistent color\n        // This is a very basic implementation\n        return 0.5; // Placeholder\n    }\n    \n    private double detectFormElements(BufferedImage image) {\n        // Look for white rectangular regions (form fields)\n        int height = image.getHeight();\n        int width = image.getWidth();\n        int whiteRegions = 0;\n        int totalRegions = 0;\n        \n        int regionSize = 20;\n        for (int y = 0; y < height - regionSize; y += regionSize / 2) {\n            for (int x = 0; x < width - regionSize; x += regionSize / 2) {\n                double avgBrightness = 0;\n                \n                for (int dy = 0; dy < regionSize; dy++) {\n                    for (int dx = 0; dx < regionSize; dx++) {\n                        Color color = new Color(image.getRGB(x + dx, y + dy));\n                        avgBrightness += (color.getRed() + color.getGreen() + color.getBlue()) / 3.0;\n                    }\n                }\n                \n                avgBrightness /= (regionSize * regionSize);\n                \n                if (avgBrightness > 200) {\n                    whiteRegions++;\n                }\n                totalRegions++;\n            }\n        }\n        \n        return (double) whiteRegions / totalRegions;\n    }\n    \n    private double detectNavigationElements(BufferedImage image) {\n        // Look for horizontal strips with consistent colors (navigation bars)\n        // This is a simplified implementation\n        return 0.3; // Placeholder\n    }\n    \n    /**\n     * Public API for searching similar screenshots\n     */\n    public List<ScreenshotNode> findSimilarScreenshots(String screenshotId, double minSimilarity) {\n        try {\n            Optional<ScreenshotNode> screenshot = screenshotRepository.findById(Long.valueOf(screenshotId));\n            if (!screenshot.isPresent() || screenshot.get().getClipEmbedding() == null) {\n                return new ArrayList<>();\n            }\n            \n            List<ScreenshotNode> similar = new ArrayList<>();\n            List<ScreenshotNode> allScreenshots = screenshotRepository.findScreenshotsWithEmbeddings();\n            \n            float[] queryEmbedding = decodeBase64ToFloatArray(screenshot.get().getClipEmbedding());\n            \n            for (ScreenshotNode candidate : allScreenshots) {\n                if (candidate.getId().equals(screenshot.get().getId()) || candidate.getClipEmbedding() == null) {\n                    continue;\n                }\n                \n                float[] candidateEmbedding = decodeBase64ToFloatArray(candidate.getClipEmbedding());\n                double similarity = calculateCosineSimilarity(queryEmbedding, candidateEmbedding);\n                \n                if (similarity >= minSimilarity) {\n                    similar.add(candidate);\n                }\n            }\n            \n            // Sort by similarity (would need to store similarity score)\n            return similar;\n            \n        } catch (Exception e) {\n            log.error(\"Failed to find similar screenshots: {}\", e.getMessage());\n            return new ArrayList<>();\n        }\n    }\n    \n    /**\n     * Batch process multiple screenshots for embeddings\n     */\n    @Async\n    public CompletableFuture<Void> batchProcessEmbeddings(List<ScreenshotNode> screenshots) {\n        try {\n            log.info(\"Starting batch processing of {} screenshots\", screenshots.size());\n            \n            // Process in batches\n            for (int i = 0; i < screenshots.size(); i += batchSize) {\n                int end = Math.min(i + batchSize, screenshots.size());\n                List<ScreenshotNode> batch = screenshots.subList(i, end);\n                \n                List<CompletableFuture<Void>> futures = batch.stream()\n                    .map(this::generateEnhancedEmbeddings)\n                    .collect(Collectors.toList());\n                \n                CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();\n                \n                log.info(\"Completed batch {}/{}\", (i / batchSize) + 1, (screenshots.size() + batchSize - 1) / batchSize);\n            }\n            \n            log.info(\"Batch processing completed for {} screenshots\", screenshots.size());\n            \n        } catch (Exception e) {\n            log.error(\"Error in batch processing: {}\", e.getMessage());\n        }\n        \n        return CompletableFuture.completedFuture(null);\n    }\n}